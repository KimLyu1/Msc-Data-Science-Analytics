---
title: "All the thing"
output: default
---

# load libraries
```{r}
library(data.table)  # for reading data
library(xts)         # for converting data type
library(fpp2)        # for forecasting
library(Hmisc)       # for checking data 
library(ggplot2)     # for plotting
library(corrplot)    # for plotting heat map
library(tseries)     # for create time series
library(Metrics)     # for evaluation

library(magrittr)    # for making code tidy
library(recipes)     # for data set pre-processing
library(keras)       # for modeling the LSTM
library(lubridate)   # for date processing
library(tidyverse)   # for tidy data
library(quantmod)    # for getting stock data
```

# load data file
```{r}
AAPL_data <- fread("AppleFinalData.csv")
MSFT_data <- fread("MicrosoftFinalData.csv")

# select variables
AAPL_data <- AAPL_data %>% select(-c("Open", "High", "Low", "Close", "Adj Close"))
MSFT_data <- MSFT_data %>% select(-c("Open", "High", "Low", "Close", "Adj Close"))

# collect the historical data from yahoo finance
getSymbols("AAPL", from = "2006-12-01", to = "2016-12-01", src = "yahoo")
getSymbols("MSFT", from = "2006-12-01", to = "2016-12-01", src = "yahoo")

# combine two data set
AAPL <- AAPL %>% as.data.frame() %>% cbind(AAPL_data)
MSFT <- MSFT %>% as.data.frame() %>% cbind(MSFT_data)

# remove the 'Date' column
AAPL <- AAPL %>% select(-Date)
MSFT <- MSFT %>% select(-Date)

# create a vector to store the column names
names <- c("Open", "High", "Low", "Close", "Volume", "Adjusted", "compound", "neg", "neu", "pos")

# update the column names
colnames(AAPL) <- names
colnames(MSFT) <- names

# split the training set and test set for later modelling
train_AAPL <- head(AAPL, 2100)
train_MSFT <- head(MSFT, 2100)
test_AAPL <- tail(AAPL, 417)
test_MSFT <- tail(MSFT, 417)
```

## Inspect data structure
```{r}
# structure
str(AAPL)
str(MSFT)

# inspect the last 6 rows
tail(AAPL)
tail(MSFT)
```
- They all have ten variables, namely Open, High, Low, Close, Volume Adjusted, compound, neg, neu and pos. All variables are of numeric type.

## Inspect whether has NA value
```{r}
describe(AAPL)      # no missing value, perfect!
describe(MSFT)      # no missing value, perfect!
```

## statistic summary
```{r}
summary(AAPL)
# the time range is between 2006-12 and 2016-11. all variables are seems right.

summary(MSFT)
# the MSFT dataset is more sensible, all variables are seems right.
```

# EDA 
## K-Chart and some indicators
```{r}

aa <- autoplot(ts(AAPL$Adjusted, frequency = 252))+
  ggtitle("AAPL")+
  theme_light() +
  ylab("Adjusted")

bb <- autoplot(ts(MSFT$Adjusted, frequency = 252))+
  ggtitle("MSFT")+
  theme_light() +
  ylab("Adjusted")

gridExtra::grid.arrange(aa, bb, ncol = 2)


chartSeries(AAPL, TA='addATR();addBBands();addCCI();addRSI()')
# AAPL is doing well, going from $5 per share to nearly $30 per share, reflecting sideways its popularity with investors. But it has not always been on the rise, and recent years have reflected a downward trend. Between 2014 and 2015, for example, its share price reached a peak, followed by a repetition of declines and rises again year on year. Overall, however, the trend is one of constant upward movement.

chartSeries(MSFT, TA='addATR();addBBands();addCCI();addRSI()')
# The size of the customer base of Microsoft, one of the earliest Internet companies, was huge and as a result there were few who invested in the company. After the financial crisis of 2008, its share price fell to around $10 for a while, but then the economy recovered and it has risen to 60 per share in recent years. For a share price, it is considerably higher than AAPL, but the general trend for MSFT is up.
```
In the previous step I applied four indicators to the plot, which are:

- *ATR* indicates Average True Range and is an indicator used to measure the volatility of prices, i.e. an indication of the rate of change in the market. In the early days it was mostly used in the futures market, but is now also used in equities, foreign exchange, etc. The higher the ATR indicator, the greater the chance of a price trend reversal, and the opposite holds true.
 
- *BBands*: Usually, during a share price turnaround, the most important thing that investors want to know must be how long the share price ticket will consolidate before it generates a market. This is because if the stock is bought too early and the stock does not rise, the utilisation of capital is reduced and the investor has to bear the risk of a fall in the share price. This is where the Bollinger Bands indicator can work its magic and give the right indication of the end of a consolidation. The indicator draws three lines on the graph, the upper and lower lines can be seen as pressure and support lines respectively, and between the two lines there is an average price line, preferably set at 20, which generally runs in a channel formed by the pressure and support lines.
 
- *CCI* Commodity Channel Index: Specially measures whether a stock price has fallen outside of its normal distribution. In general, buy when CCI rises above 100; sell when CCI falls below -100.
 
- *RSI*: The Relative Strength Index RSI is a measure of the speed and variability of price movements. and oscillates between 0 and 100. In general buy when the RSI value is above 70 and sell when it is below 30.

## understand variables
```{r}
# first create a new column: stat, which is the symbol of each stock 
AAPL <- AAPL %>% mutate(Stock = "AAPL")
MSFT <- MSFT %>% mutate(Stock = "MSFT")

# combine them for later plotting 
BOTH <- rbind(AAPL, MSFT)

# the difference of Adjusted
ggplot(BOTH, aes(x = Stock, y = Adjusted, fill = Stock)) + 
  geom_boxplot() +
  theme_light() +
  ggtitle("The difference of Adjusted between AAPL and MSFT") + 
  xlab("Stock Name") 
# As you can see the difference between AAPL and MSFT on Adjusted is still significant. both AAPL and MSFT are unstable, but the average price of MSFT is much higher than that of AAPL.
```

```{r}
# Open, high, Low distribution  --AAPL
options(repr.plot.width = 20, repr.plot.height = 20)
a <- ggplot(AAPL, aes(x = Open)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of AAPL Opening Price")
b <- ggplot(AAPL, aes(x = High)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of AAPL Highest Price")
c <- ggplot(AAPL, aes(x = Low)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of AAPL Lowest Price")

# Open, high, Low distribution  --MSFT
options(repr.plot.width = 20, repr.plot.height = 20)
d <- ggplot(MSFT, aes(x = Open)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of MSFT Opening Price")
e <- ggplot(MSFT, aes(x = High)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of MSFT Highest Price")
f <- ggplot(MSFT, aes(x = Low)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Density Plot of MSFT Lowest Price")
```

```{r}
# plot
gridExtra::grid.arrange(a, b, c, ncol = 3, nrow = 1)   # execute it in console will be precised
gridExtra::grid.arrange(d, e, f, ncol = 3, nrow = 1)   # execute it in console will be precised

# All three prices for AAPL show a skewed distribution, indicating that most of the time it is around $5
# MSFT presents a similar picture
```

```{r}
# sentiment variables  -- AAPL
options(repr.plot.width = 20, repr.plot.height = 20)
g <- ggplot(AAPL, aes(x = neu)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about AAPL Neutral Sentiment Score")
h <- ggplot(AAPL, aes(x = neg)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about AAPL Negative Sentiment Score")
i <- ggplot(AAPL, aes(x = pos)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about AAPL Positive Sentiment Score")

# sentiment variables  -- MSFT
options(repr.plot.width = 20, repr.plot.height = 20)
j <- ggplot(MSFT, aes(x = neu)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about MSFT Neutral Sentiment Score")
k <- ggplot(MSFT, aes(x = neg)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about MSFT Negative Sentiment Score")
l <- ggplot(MSFT, aes(x = pos)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about MSFT Positive Sentiment Score")
```

```{r}
# plot
gridExtra::grid.arrange(g, h, i, ncol = 3, nrow = 1)   # execute it in console will be precised
gridExtra::grid.arrange(j, k, l, ncol = 3, nrow = 1)   # execute it in console will be precised

# It seems that the news media will discuss AAPL more than MSFT. But for the most part, the news media has been neutral on both stocks, whether it is AAPL or MSFT.
```

```{r}
m <- ggplot(AAPL, aes(x = compound)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about AAPL Compound Sentiment Score")
n <- ggplot(MSFT, aes(x = compound)) + 
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("News about MSFT Compound Sentiment Score")

gridExtra::grid.arrange(m, n, ncol = 2, nrow = 1)
# The compound scores give a less mixed picture, with a mostly positive news attitude towards the AAPL and a mostly neutral news attitude towards the MSFT.
```

```{r}
# correlation
AAPL_numVar <- AAPL %>% select(-Stock)
MSFT_numVar <- MSFT %>% select(-Stock)

cor_AAPL <- cor(AAPL_numVar)
cor_MSFT <- cor(MSFT_numVar)

# plot
corrplot(cor_AAPL)
corrplot(cor_MSFT)
# The correlation between sentiment scores and stock prices can be found to be small
```

# ARIMA preparation

## 1.Testing and ensuring stationary

To model a time series using the ARIMA method, the series must be smooth. A smooth time series represents a time series with no trend, where one has a constant mean and variance over time, which makes it easy to predict values.

*Testing stationary* - We test for stationary using the Augmented Dickey-Fuller unit root test. For a smooth time series, the p-value obtained by the ADF test must be less than 0.05 or 5%. If the p-value is greater than 0.05 or 5%, it can be concluded that the time series has a unit root, which means that it is a non-stationary process.

*Differencing* -In order to convert a non-smooth process into a smooth one, we apply the difference method. Differentiating time series means finding the differences between successive values of the time series data. The difference values form a new time series data set, which can be tested to find new correlations or other interesting statistical properties.

We can apply the difference method several times in succession to produce "first order differences", "second order differences", etc.

Before we proceed to the next step, we apply the appropriate order of difference (d) to smooth the time series.

```{r}
adf.test(AAPL$Adjusted)    # AAPL's Adjusted is not a stable sequence
adf.test(MSFT$Adjusted)    # MSFT's Adjusted is not a stable sequence
```

```{r}
# Differentiate the sequence to smooth the sequence (here we start using the training set)
## AAPL
AAPL_adj <- ts(train_AAPL$Adjusted, frequency = 252)
AAPL_adj_diff <- AAPL_adj %>% diff() 


# plot and adf test
plot(AAPL_adj_diff)
adf.test(AAPL_adj_diff)   # p-value less than 0.05, so it's a stable sequence

## MSFT 
MSFT_adj <- ts(train_MSFT$Adjusted, frequency = 252)
MSFT_adj_diff <- MSFT_adj %>% diff() 


# plot and adf test
plot(MSFT_adj_diff)
adf.test(MSFT_adj_diff)   # p-value less than 0.05, so it's a stable sequence
```

## 2.Identifying p and q

In this step, we determine the appropriate order for the autoregressive (AR) and moving average (MA) processes by using the autocorrelation function (ACF) and the partial autocorrelation function (PACF).

*Identifying the p-order of an AR model*

For the AR model, the ACF will decay exponentially and the PACF will be used to identify the order (p) of the AR model. If we have a significant peak at lag 1 on the PACF, then we have an AR model of order 1, i.e. AR(1). If we have significant peaks at lags 1,2 and 3 on the PACF then we have a 3rd order AR model, i.e. AR(3). (usually only the first 5 orders are looked at)

*Identifying the q-order of the MA model*

For the MA model, the PACF will decay exponentially and the ACF plot will be used to identify the order of the MA process. If we have a significant peak at lag 1 on the ACF, then we have an MA model of order 1, i.e. MA(1). If we have significant peaks at lags 1,2 and 3 on the ACF then we have an MA model of order 3, i.e. MA(3). (usually only the first 5 orders are looked at)
```{r}
## AAPL
acf(AAPL_adj_diff, main = "AAPL ACF Plot")    # There is a significant peak at lag 1, i.e. MA = 1 and q is 1
pacf(AAPL_adj_diff, main = "AAPL PACF Plot")  
# There are significant peaks at lags 4, 5 respectively, i.e. AR = 2, p for 2

## MSFT
acf(MSFT_adj_diff, main = "MSFT ACF Plot")    # There is a significant peak at lag 1, i.e. MA = 1 and q is 1
pacf(MSFT_adj_diff, main = "MSFT PACF Plot")   # There is a significant peak at lag 4, i.e. AR = 1 and p is 1
```
## 3.Estimates and forecasts

Once we have determined the parameters (p, d, q), we can estimate the accuracy of the ARIMA model on the training dataset and then use the fitted model to predict the values of the test dataset using the prediction function. Finally, we cross-check that our predicted values are consistent with the actual values.
```{r}
## AAPL
### Constructed using automatic selection parameters
AAPL_autoArima <- auto.arima(AAPL_adj)
AAPL_autoArima
checkresiduals(AAPL_autoArima, lag = 5)   # It's white noise

### Constructed using manually selected parameters
AAPL_Arima <- Arima(AAPL_adj, order = c(2,1,1))
AAPL_Arima
checkresiduals(AAPL_Arima, lag = 5)   # It's white noise

## MSFT
### Constructed using automatic selection parameters
MSFT_autoArima <- auto.arima(MSFT_adj)
MSFT_autoArima
checkresiduals(MSFT_autoArima, lag = 5)   # It's white noise

### Constructed using manually selected parameters
MSFT_Arima <- Arima(MSFT_adj, order = c(1,1,1))
MSFT_Arima
checkresiduals(MSFT_Arima, lag = 5)   # It's white noise
```

## 4.Use of model predictions
```{r}
# AAPL 
## auto.arima
AAPL_autoArimaforecast <- forecast(AAPL_autoArima, h = 417)    # the length of test set
plot(AAPL_autoArimaforecast)
x <- autoplot(AAPL_autoArimaforecast, series = "Predicted", color = "blue") +
  autolayer(ts(AAPL$Adjusted, frequency = 252), series = "Actual") +
  theme_light() +
  ylab("AAPL Adjusted Closing price")


## Arima manually
AAPL_Arimaforecast <- forecast(AAPL_Arima, h = 417)    # the length of test set
plot(AAPL_Arimaforecast)
y <- autoplot(AAPL_Arimaforecast, series = "Predicted", color = "blue") +
  autolayer(ts(AAPL$Adjusted, frequency = 252), series = "Actual") +
  theme_light() +
  ylab("AAPL Adjusted Closing price")

gridExtra::grid.arrange(x, y, ncol = 2)

# MSFT 
## auto.arima
MSFT_autoArimaforecast <- forecast(MSFT_autoArima, h = 417)    # the length of test set
plot(MSFT_autoArimaforecast)
autoplot(MSFT_autoArimaforecast, series = "Predicted", color = "blue") +
  autolayer(ts(MSFT$Adjusted, frequency = 252), series = "Actual") +
  theme_light() +
  ylab("MSFT Adjusted Closing price")

## Arima manually
MSFT_Arimaforecast <- forecast(MSFT_Arima, h = 417)    # the length of test set
plot(MSFT_Arimaforecast)
autoplot(MSFT_Arimaforecast, series = "Predicted", color = "blue") +
  autolayer(ts(MSFT$Adjusted, frequency = 252), series = "Actual") +
  theme_light() +
  ylab("MSFT Adjusted Closing price")
```

## 5.Test the distribution of their predicted residuals
```{r message=FALSE, warning=FALSE}
# AAPL
## auto.arima
ggplot(data.frame(Residuals = AAPL_autoArimaforecast$residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Residuals of Auto.Arima Model AAPL")

## Arima manually
ggplot(data.frame(Residuals = AAPL_Arimaforecast$residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Residuals of Arima Model AAPL")


# MSFT
## auto.arima
ggplot(data.frame(Residuals = MSFT_autoArimaforecast$residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Residuals of Auto.Arima Model MSFT")

## Arima manually
ggplot(data.frame(Residuals = MSFT_Arimaforecast$residuals), aes(x = Residuals)) +
  geom_histogram(bins = 30,aes(y=..density..),colour="black", fill="grey") +
  geom_density(alpha=.2,fill="blue") +
  theme_light() +
  ggtitle("Residuals of Arima and Auto.arima Model MSFT")
```
The forecast errors appear to be normally distributed with zero mean and constant variance, and the ARIMA model does appear to provide an appropriate forecasting model.

## 6.Check the accuracy of the model
```{r}
# Accuracy
## AAPL
cat("The RMSE of AAPL use auto.arima:", rmse(test_AAPL$Adjusted, AAPL_autoArimaforecast$mean), "\n")
cat("The RMSE of AAPL use Arima:", rmse(test_AAPL$Adjusted, AAPL_Arimaforecast$mean), "\n")

## MSFT
cat("The RMSE of MSFT use auto.arima:", rmse(test_MSFT$Adjusted, MSFT_autoArimaforecast$mean), "\n")
cat("The RMSE of MSFT use Arima:", rmse(test_MSFT$Adjusted, MSFT_Arimaforecast$mean))
```

```{r}
# plot
test_AAPL <- test_AAPL %>% mutate(test_AAPL, pred = AAPL_Arimaforecast$mean)
test_MSFT <- test_MSFT %>% mutate(test_MSFT, pred = MSFT_Arimaforecast$mean)

ggplot(test_AAPL, aes(x = ymd(rownames(test_AAPL)))) +
  geom_line(aes(y = Adjusted, color = "Actual")) +
  geom_line(aes(y = pred, color = "Prediction")) +
  theme_light() +
  xlab("Date") +
  ylab("Price") +
  scale_colour_manual("",
                      breaks = c("Actual","Prediction"),
                      values = c("Actual"="gray","Prediction"="orange")) +
  theme(legend.position = "top") +
  ggtitle("AAPL")

ggplot(test_MSFT, aes(x = ymd(rownames(test_MSFT)))) +
  geom_line(aes(y = Adjusted, color = "Actual")) +
  geom_line(aes(y = pred, color = "Prediction")) +
  theme_light() +
  xlab("Date") +
  ylab("Price") +
  scale_colour_manual("",
                      breaks = c("Actual","Prediction"),
                      values = c("Actual"="gray","Prediction"="orange")) +
  theme(legend.position = "top") +
  ggtitle("MSFT")
```


# LSTM Model
############################################################################################
Now we create the transformation function for later modeling.

# Define functions
```{r}
# Note: the functions create in this chunk can be found here: https://github.com/RJHKnight/MultiVariateLSTMWithKeras/blob/master/Utils.R

# Reshape and expand
reshapeForLSTM <- function(originalDF, numTimeSteps, columnsToExclude) {

  originalColumnsToExclude <- columnsToExclude
  
  # Create new columns based on previous steps
  for (t in 1:(numTimeSteps)) {
    
    originalDF %<>%
      mutate_at(vars(-one_of(columnsToExclude)), funs(lagTmp = lag(., n = t))) %>%
      rename_at(vars(contains("lagTmp")), ~str_replace_all(., "lagTmp", as.character(t)))
    
    columnsToExclude <- c(columnsToExclude, colnames(originalDF)[str_detect(colnames(originalDF), "_")])
  }
  
  res_matrix <- as.matrix(select(originalDF, -originalColumnsToExclude))
  
  res_matrix <- res_matrix[complete.cases(res_matrix),]
  
  dim(res_matrix) <- c(nrow(res_matrix),numTimeSteps+1,ncol(res_matrix)/(numTimeSteps+1))
  
  return (originalDF)
}

reshapeForLSTMLoop <- function(originalDF, numTimeSteps, columnsToExclude) {
  
  originalDF <- as.data.frame(originalDF)
  
  colNames <- colnames(originalDF)
  newMatrix <- as.matrix(originalDF[,!colNames %in% columnsToExclude])
  
  for (i in 1:ncol(originalDF)) {
    
    thisColName <- colNames[i]
    
    if (thisColName %in% columnsToExclude) {
      next
    }
    
    laggedColumns <- sapply(1:numTimeSteps, function(x) {
      c(rep(NA, x), head(originalDF[,i], -x))
    })
    
    colnames(laggedColumns) <- paste0(thisColName, "_", 1:numTimeSteps)
    
    newMatrix <- cbind(newMatrix, laggedColumns)
  }
  
  # Remove NAs
  newMatrix <- newMatrix[complete.cases(newMatrix),]
  
  dim(newMatrix) <- c(nrow(newMatrix),numTimeSteps+1,ncol(newMatrix)/(numTimeSteps+1))
  return (newMatrix)
}
```
# AAPL
## Read the data and simple manipulation
```{r}
# setting random number seed
set.seed(1234)

# read csv file
AAPL <- read.csv("D:/RFile/CS5500/AAPL and microsoft stock and sentiment/FinalData.csv")

# filter data to specify AAPL
AAPL <- AAPL %>% filter(Stock == "AAPL") %>% select(-Stock)

# get the next price of the day
nextClose <- data.frame(AAPL$Adjusted[2:nrow(AAPL)]) %>% rbind(NA)

# combine with original data set
aapl <- cbind(AAPL, nextClose)
colnames(aapl)[12] <- "nextClose"

# remove the variables do not use
aapl <- na.omit(aapl) %>% select(-Close)

# convert the Date into appropriate data type
aapl$Date <- ymd(aapl$Date)

# split the train and test set. Because it is a time-series data, so split in ascending order.
aapl_train <- aapl[1:2100, ]
aapl_test <- aapl[2101:nrow(aapl), ]
```

## LSTMN t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)

train_X <- as.matrix(train_data[,-6])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-6])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_n <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE_n[i] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_n <- min(RMSE_n)
RMSE_n <- RMSE_n %>% as.data.frame() %>% filter(. < 2 * min_RMSE_n)
avg_RMSE_n <- sum(RMSE_n)/nrow(RMSE_n)
cat("The Average RMSE is:", avg_RMSE_n)
```

## LSTMP t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + neg + neu + pos, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)

train_X <- as.matrix(train_data[,-9])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-9])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_p <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE_p[i] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_p <- min(RMSE_p)
RMSE_p <- RMSE_p %>% as.data.frame() %>% filter(. < 2 * min_RMSE_p)
avg_RMSE_p <- sum(RMSE_p)/nrow(RMSE_p)
cat("The Average RMSE is:", avg_RMSE_p)
```

## LSTMC t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)

train_X <- as.matrix(train_data[,-7])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-7])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_c <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE_c[i] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_c <- min(RMSE_c)
RMSE_c <- RMSE_c %>% as.data.frame() %>% filter(. < 2 * min_RMSE_c)
avg_RMSE_c <- sum(RMSE_c)/nrow(RMSE_c)
cat("The Average RMSE is:", avg_RMSE_c)
```

## LSTMA t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound + neg + neu + pos, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)

train_X <- as.matrix(train_data[,-10])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-10])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_a <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE_a[i] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE <- min(RMSE_a)
RMSE_a <- RMSE_a %>% as.data.frame() %>% filter(. < 2 * min_RMSE)
avg_RMSE_a <- sum(RMSE_a)/nrow(RMSE_a)
cat("The Average RMSE is:", avg_RMSE_a)
```

## LSTMN t-2 ~ t-5

### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()


for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 10, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
n_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(n_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(n_timestep_table) <- c("avg_RMSE_n")
n_timestep_table <- t(n_timestep_table) %>% data.frame()
n_timestep_table
```

## LSTMP t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + neg + neu + pos, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()


for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 10, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
p_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(p_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(p_timestep_table) <- c("avg_RMSE_p")
p_timestep_table <- t(p_timestep_table) %>% data.frame()
p_timestep_table
```


## LSTMC t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()

for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 10, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
c_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(c_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(c_timestep_table) <- c("avg_RMSE_c")
c_timestep_table <- t(c_timestep_table) %>% data.frame()
c_timestep_table
```

## LSTMA t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound + neg + neu + pos, data = aapl) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = aapl)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = aapl_train)
test_data  <- bake(trained_rec, new_data = aapl_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()

for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 10, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
aapl_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - aapl_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
a_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(a_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(a_timestep_table) <- c("avg_RMSE_a")
a_timestep_table <- t(a_timestep_table) %>% data.frame()
a_timestep_table
```

## AAPL RMSE table
```{r}
# t-1
aapl_t_1_table <- rbind(avg_RMSE_n, avg_RMSE_p, avg_RMSE_c, avg_RMSE_a) %>% as.data.frame()
colnames(aapl_t_1_table) <- c("t.1")
aapl_t_1_table

# t-1 ~ t-5
AAPL_RMSE_table <- cbind(aapl_t_1_table, rbind(n_timestep_table, p_timestep_table, c_timestep_table, a_timestep_table))
```

# MSFT
## Read the data and simple manipulation
```{r}
# setting random number seed
set.seed(1234)

# read csv file
MSFT <- read.csv("D:/RFile/CS5500/AAPL and microsoft stock and sentiment/FinalData.csv")

# filter data to specify MSFT
MSFT <- MSFT %>% filter(Stock == "MSFT") %>% select(-Stock)

# get the next price of the day
nextClose <- data.frame(MSFT$Adjusted[2:nrow(MSFT)]) %>% rbind(NA)

# combine with original data set
microsoft <- cbind(MSFT, nextClose)
colnames(microsoft)[12] <- "nextClose"

# remove the variables do not use
microsoft <- na.omit(microsoft) %>% select(-Close)

# convert the Date into appropriate data type
microsoft$Date <- ymd(microsoft$Date)

# split the train and test set. Because it is a time-series data, so split in ascending order.
microsoft_train <- microsoft[1:2100, ]
microsoft_test <- microsoft[2101:nrow(microsoft), ]
```

## LSTMN t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)

train_X <- as.matrix(train_data[,-6])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-6])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_n <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE_n[i] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_n <- min(RMSE_n)
RMSE_n <- RMSE_n %>% as.data.frame() %>% filter(. < 2 * min_RMSE_n)
avg_RMSE_n <- sum(RMSE_n)/nrow(RMSE_n)
cat("The Average RMSE is:", avg_RMSE_n)
```

## LSTMP t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + neg + neu + pos, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)

train_X <- as.matrix(train_data[,-9])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-9])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_p <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE_p[i] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_p <- min(RMSE_p)
RMSE_p <- RMSE_p %>% as.data.frame() %>% filter(. < 2 * min_RMSE_p)
avg_RMSE_p <- sum(RMSE_p)/nrow(RMSE_p)
cat("The Average RMSE is:", avg_RMSE_p)
```

## LSTMC t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)

train_X <- as.matrix(train_data[,-7])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-7])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_c <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE_c[i] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_c <- min(RMSE_c)
RMSE_c <- RMSE_c %>% as.data.frame() %>% filter(. < 2 * min_RMSE_c)
avg_RMSE_c <- sum(RMSE_c)/nrow(RMSE_c)
cat("The Average RMSE is:", avg_RMSE_c)
```

## LSTMA t-1
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound + neg + neu + pos, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)

train_X <- as.matrix(train_data[,-10])
train_Y <- as.matrix(train_data$nextClose)
test_X <- as.matrix(test_data[,-10])
test_Y <- as.matrix(test_data$nextClose)

dim(train_X) <- c(nrow(train_X),1,ncol(train_X))
dim(test_X) <- c(nrow(test_X),1,ncol(test_X))
```

### Modelling
```{r warning=FALSE}
RMSE_a <- c()

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- pred * ranges[2,4]

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE_a[i] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}

# calculate the Average RMSE
min_RMSE_a <- min(RMSE_a)
RMSE_a <- RMSE_a %>% as.data.frame() %>% filter(. < 2 * min_RMSE_a)
avg_RMSE_a <- sum(RMSE_a)/nrow(RMSE_a)
cat("The Average RMSE is:", avg_RMSE_a)
```

## LSTMN t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()


for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 25, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
n_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(n_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(n_timestep_table) <- c("avg_RMSE_n")
n_timestep_table <- t(n_timestep_table) %>% data.frame()
n_timestep_table
```

## LSTMP t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + neg + neu + pos, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()

for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
p_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(p_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(p_timestep_table) <- c("avg_RMSE_p")
p_timestep_table <- t(p_timestep_table) %>% data.frame()
p_timestep_table
```

## LSTMC t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()

for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
c_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(c_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(c_timestep_table) <- c("avg_RMSE_c")
c_timestep_table <- t(c_timestep_table) %>% data.frame()
c_timestep_table
```

## LSTMA t-2 ~ t-5
### Data processing
```{r}
trained_rec <- recipe(nextClose ~ Open + High + Low + Adjusted + Volume + compound + neg + neu + pos, data = microsoft) %>%  # assign the variables to be used
  step_range(all_numeric()) %>%    # scale and centre variables in range[0,1]
  prep(training = microsoft)# estimate the required parameters from a training set that can be later applied to other data sets.

# execute the pre-processing steps
train_data <- bake(trained_rec, new_data = microsoft_train)
test_data  <- bake(trained_rec, new_data = microsoft_test)
```

### Modelling
```{r warning=FALSE}
RMSE <- data.frame()
table <- data.frame()

for (j in 1:4){
  # create the matrix which meet the requirement of LSTM using 2 defined functions. Change the numTimeSteps you need.
train_X <- reshapeForLSTMLoop(train_data, numTimeSteps = j, columnsToExclude = "nextClose")
train_Y <- as.matrix(tail(train_data$nextClose, -j))
test_X <- reshapeForLSTMLoop(test_data, numTimeSteps = j, columnsToExclude = "nextClose")
test_Y <- as.matrix(tail(test_data$nextClose, -j))

for (i in 1:2){
model <- keras_model_sequential()

# set the layers
model %>%
  layer_lstm(units = 64, 
             input_shape = c(dim(train_X)[2], dim(train_X)[3]),
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 32, 
             return_sequences = TRUE,
             activation = "relu") %>%
  layer_lstm(units = 16,
             return_sequences = FALSE,
             activation = "relu") %>%
  layer_dense(units = 1,
              activation = "sigmoid")

# compile
model %>%
  compile(loss = "mse", optimizer = "adam")

# fit
model %>%
  fit(train_X, 
      train_Y, 
      epochs = 15, 
      batch_size = 30,
      validation_data = list(test_X, test_Y), 
      verbose = 1,
      shuffle = FALSE)

# predict value
pred <- model %>% predict(test_X)

# call the original values
ranges <- trained_rec$steps[[1]]$ranges

# inverse normalize
pred <- c(rep(NA, j), pred * ranges[2,4])

# add new column for later plotting
microsoft_test$predictedValue <- pred

# calculate the RMSE
RMSE[i,j] <- sqrt(mean((pred - microsoft_test$nextClose)^2, na.rm = TRUE))
}
}

# the minimum RMSE
min_RMSE <- apply(RMSE, 2, min) %>% as.data.frame()

# convert to data frame
for (k in 1:4) {
after_filter <- filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
table[1:nrow(after_filter), k] <- after_filter #filter(RMSE[k], RMSE[k] < 2 * min_RMSE[k,1])
}

# calculate the Average RMSE
a_timestep_table <- apply(table, 2, mean, na.rm = T) %>% data.frame()
rownames(a_timestep_table) <- c("t-2", "t-3", "t-4", "t-5")
colnames(a_timestep_table) <- c("avg_RMSE_a")
a_timestep_table <- t(a_timestep_table) %>% data.frame()
a_timestep_table
```

























## MSFT RMSE Table
```{r}
# t-1
msft_t_1_table <- rbind(avg_RMSE_n, avg_RMSE_p, avg_RMSE_c, avg_RMSE_a) %>% as.data.frame()
colnames(msft_t_1_table) <- c("t.1")

# t-1 ~ t-5
MSFT_RMSE_table <- cbind(msft_t_1_table, rbind(n_timestep_table, p_timestep_table, c_timestep_table, a_timestep_table))

MSFT_RMSE_table
```

